{"url": "C:\\Users\\33427\\AppData\\Local\\Temp\\gradio\\9cf7442d962d811a018c5067f242e5161cf4d61ee12af825f44cb29a84e03f2d\\qwen.txt", "raw": [{"content": "# Quickstart\n\nThis guide helps you quickly start using Qwen3. \nWe provide examples of [Hugging Face Transformers](https://github.com/huggingface/transformers) as well as [ModelScope](https://github.com/modelscope/modelscope), and [vLLM](https://github.com/vllm-project/vllm) for deployment.\n\nYou can find Qwen3 models in [the Qwen3 collection](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f) at Hugging Face Hub and [the Qwen3 collection](https://www.modelscope.cn/collections/Qwen3-9743180bdc6b48) at ModelScope.\n\n## Transformers\n\nTo get a quick start with Qwen3, you can try the inference with `transformers` first.\nMake sure that you have installed `transformers>=4.51.0`.\nWe advise you to use Python 3.10 or higher, and PyTorch 2.6 or higher.\n\nThe following is a very simple code snippet showing how to run Qwen3-8B:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-8B\"\n\n# load the tokenizer and the model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language models.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt},\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True, # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parse thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\n```\n\nQwen3 will think before respond, similar to QwQ models.\nThis means the model will use its reasoning abilities to enhance the quality of generated responses.\nThe model will first generate thinking content wrapped in a `<think>...</think>` block, followed by the final response.\n\n-   Hard Switch:\n    To strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models, you can set `enable_thinking=False` when formatting the text. \n    ```python\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n        enable_thinking=False,  # Setting enable_thinking=False disables thinking mode\n    )\n    ```\n    It can be particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n-   Soft Switch:\n    Qwen3 also understands the user's instruction on its thinking behavior, in particular, the soft switch `/think` and `/no_think`.\n    You can add them to user prompts or system messages to switch the model's thinking mode from turn to turn. \n    The model will follow the most recent instruction in multi-turn conversations.\n\n:::{note}\nFor thinking mode, use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0 (the default setting in `generation_config.json`).\nDO NOT use greedy decoding, as it can lead to performance degradation and endless repetitions. \nFor more detailed guidance, please refer to the Best Practices section.\n\nFor non-thinking mode, we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0. \n:::\n\n\n## ModelScope\n\nTo tackle with downloading issues, we advise you to try [ModelScope](https://github.com/modelscope/modelscope).\nBefore starting, you need to install `modelscope` with `pip`. \n\n`modelscope` adopts a programmatic interface similar (but not identical) to `transformers`.\nFor basic usage, you can simply change the first line of code above to the following:\n\n```python\nfrom modelscope import AutoModelForCausalLM, AutoTokenizer\n```\n\nFor more information, please refer to [the documentation of `modelscope`](https://www.modelscope.cn/docs).\n\n## vLLM \n\nvLLM is a fast and easy-to-use framework for LLM inference and serving. \nIn the following, we demonstrate how to build a OpenAI-API compatible API service with vLLM.\n\nFirst, make sure you have `vllm>=0.8.5` installed.\n\nRun the following code to build up a vLLM service. \nHere we take Qwen3-8B as an example:\n\n```bash\nvllm serve Qwen/Qwen3-8B --enable-reasoning --reasoning-parser deepseek_r1\n```\n\nThen, you can use the [create chat interface](https://platform.openai.com/docs/api-reference/chat/completions/create) to communicate with Qwen:\n\n::::{tab-set}\n\n:::{tab-item} curl\n```shell\ncurl http://localhost:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n  \"model\": \"Qwen/Qwen3-8B\",\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"Give me a short introduction to large language models.\"}\n  ],\n  \"temperature\": 0.6,\n  \"top_p\": 0.95,\n  \"top_k\": 20,\n  \"max_tokens\": 32768\n}'\n```\n:::\n\n:::{tab-item} Python\nYou can use the API client with the `openai` Python SDK as shown below:\n\n```python\nfrom openai import OpenAI\n# Set OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nchat_response = client.chat.completions.create(\n    model=\"Qwen/Qwen3-8B\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Give me a short introduction to large language models.\"},\n    ],\n    max_tokens=32768,\n    temperature=0.6,\n    top_p=0.95,\n    extra_body={\n        \"top_k\": 20,\n    }\n)\nprint(\"Chat response:\", chat_response)\n```\n::::\n\nWhile the soft switch is always available, the hard switch is also available in vLLM through the following configuration to the API call.\nFor more usage, please refer to [our document on vLLM](../deployment/vllm).\n\n\n## Next Step\n\nNow, you can have fun with Qwen3 models. \nWould love to know more about its usage? \nFeel free to check other documents in this documentation.", "metadata": {"source": "C:\\Users\\33427\\AppData\\Local\\Temp\\gradio\\9cf7442d962d811a018c5067f242e5161cf4d61ee12af825f44cb29a84e03f2d\\qwen.txt", "title": "qwen.txt", "chunk_id": 0}, "token": 1595}], "title": "qwen.txt"}